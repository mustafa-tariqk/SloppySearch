{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yo what's good\n",
    "\n",
    "I tried to make this under 100ms for 1 billion vectors but I'm on a macbook air \n",
    "here so I'm hoping I get cut a break.\n",
    "\n",
    "\n",
    "I've experimented with KDTrees and locally sensitive hashing, LSH has proven to\n",
    "be faster. I think I may go down a merged route where one does LSH then groups\n",
    "are KDTrees? But that may hit some diminishing returns. Concurrency didn't\n",
    "make things much better for me. But that may just be my computer.\n",
    "\n",
    "I've read papers on google ScaNN and meta FAISS. This stuff goes deeper than\n",
    "I thought. Still though. I belive people jump to fancy stuff quick and don't\n",
    "have fun with numpy arrays for a bit to check if stuff even works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import sloppy_search as ss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "DB = ss.Database()\n",
    "SIZE = np.int32\n",
    "VECTORS = 2 ** 22 # ~5M\n",
    "DIMENSION = 2 ** 9 # ~500\n",
    "RESULTS = 2 ** 2 # ~5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a query vector\n",
    "query_vector = np.random.uniform(low=np.iinfo(SIZE).min,\n",
    "                                 high=np.iinfo(SIZE).max, \n",
    "                                 size=DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate database\n",
    "vectors = np.random.uniform(low=np.iinfo(SIZE).min, \n",
    "                            high=np.iinfo(SIZE).max, \n",
    "                            size=(VECTORS, DIMENSION))\n",
    "DB.add_vectors(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search database, goal - approx 100ms\n",
    "distances, indices = DB.search(query_vector, RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 2444717, Distance: 35096917107.99433\n",
      "Index: 3702668, Distance: 35098819573.292946\n",
      "Index: 1803738, Distance: 35126481656.39757\n",
      "Index: 1673384, Distance: 35185650927.646645\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "for i in range(RESULTS):\n",
    "    print(f'Index: {indices[i]}, Distance: {distances[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
